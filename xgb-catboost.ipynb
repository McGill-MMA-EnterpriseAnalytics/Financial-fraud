{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fraud Detection Predictive Model**\n",
    "Goal: Build 2 separate classifiers to be stacked alongside each other subsequently for a stronger predictive model. \n",
    "\n",
    "TabNet will be used as one of the models, which supports class weighting through loss_fn parameter. Thus, the second model should also support class weighting directly for stacking to work effectively.\n",
    "\n",
    "To complement TabNet strengths, gradient boosting methods emerge as the top candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "#!pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Load mcc data\n",
    "json_file_path = \"/Users/jiajue/Documents/McGill/Winter Term/INSY695/Group project/Fraud data/mcc_codes.json\"\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    mcc = json.load(file)\n",
    "\n",
    "# Convert to df\n",
    "mcc = pd.DataFrame.from_dict(mcc, orient='index').reset_index()\n",
    "mcc['index'] = mcc['index'].astype(int)\n",
    "\n",
    "# Merge df and mcc into single df\n",
    "df = df.merge(mcc, left_on='mcc', right_on='index', how='left')\n",
    "df = df.rename(columns={0: 'merchant information'})\n",
    "df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive stats for each column\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract day of week data for each observation for subsequent individual purchase behaviour assessment\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['transaction_day'] = df['date'].dt.dayofweek\n",
    "\n",
    "# Group by date and count fraud\n",
    "frauds_time = df.groupby(pd.Grouper(key='date', freq='Y'))['Target'].sum()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=frauds_time.index, y=frauds_time.values)\n",
    "plt.title('Fraud Cases Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Fraud Cases')\n",
    "plt.show()\n",
    "\n",
    "# No value in keeping date as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns\n",
    "df = df.drop(df.columns[[0,3,5,8,12,13]], axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature engineering**\n",
    "\n",
    "#### Assess from merchant pov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by mcc and calculate overall fraud rate\n",
    "fraud_mcc = df.groupby('merchant information')['Target'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Visualise fraud rate for each merchant\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=fraud_mcc.index, y=fraud_mcc.values)\n",
    "plt.title('Fraud by MCC')\n",
    "plt.xlabel('MCC')\n",
    "plt.ylabel('Fraud Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary col for high risk mcc - disregarded this in favour of a risk score below\n",
    "\n",
    "# Filter out high risk mcc according to fraud rate with top 10% as cut off, and transaction amount with whisker as cut off, and the freq of fraud \n",
    "def high_risk_mcc_by_transact_type(df):\n",
    "    \"\"\"\n",
    "    Filters out MCCs with a high risk of fraud considering transaction types using upper whisker value as the benchmark cut off\n",
    "    Rationale: Amount quartiles are different across transaction types (refer to EDA) and thus, the high risk mcc can be significantly different from 1 to the other\n",
    "    Returns: List of MCCs to filter out.\n",
    "    \"\"\"\n",
    "    transaction_types =['Chip Transaction', 'Online Transaction', 'Swipe Transaction']\n",
    "    high_risk_mccs = []\n",
    "\n",
    "    for t in transaction_types:\n",
    "        # Split into respective transaction type\n",
    "        df_type = df[df['use_chip_' + t] == 1]\n",
    "        # Calculate fraud rate for each mcc\n",
    "        fraud_mcc = df_type.groupby('merchant information')['Target'].mean().sort_values(ascending=False)\n",
    "        # Filter out high risk mccs in the top 10%\n",
    "        high_risk_mccs_fraud = fraud_mcc.head(int(len(fraud_mcc) * 0.1)).index.to_list()\n",
    "\n",
    "        # Calculate quartiles for amounts per merch\n",
    "        mcc_quartiles = df_type.groupby('merchant information')['amount'].quantile([0.25, 0.75]).unstack()\n",
    "        mcc_quartiles.columns = ['Q1', 'Q3']\n",
    "        mcc_quartiles['IQR'] = mcc_quartiles['Q3'] - mcc_quartiles['Q1']\n",
    "        mcc_quartiles['amount_threshold'] = mcc_quartiles['Q3'] + 1.5 * mcc_quartiles['IQR'] # upper whisker value\n",
    "\n",
    "        # Count occurence of high transaction amounts per merchant\n",
    "        df_merged = pd.merge(df_type, mcc_quartiles, on='merchant information', how='left')\n",
    "        df_merged['high_amount'] = (df_merged['amount'] >= df_merged['amount_threshold']).astype(int)\n",
    "        high_amount_counts = df_merged.groupby('merchant information')['high_amount'].sum().reset_index()\n",
    "        high_amount_counts.rename(columns={'high_amount': 'high_amount_count'}, inplace=True)\n",
    "\n",
    "        # Calculate the ratio of high transaction amount relative to total transactions\n",
    "        total_transaction_counts = df_type.groupby('merchant information').size().reset_index(name='total_transactions') # number of transactions per merchant\n",
    "        mcc_ratios = pd.merge(high_amount_counts, total_transaction_counts, on='merchant information')\n",
    "        mcc_ratios['high_amount_ratio'] = mcc_ratios['high_amount_count'] / mcc_ratios['total_transactions']\n",
    "\n",
    "        # Filter merchant on threshold of 75%\n",
    "        high_risk_mccs_amt = mcc_ratios[mcc_ratios['high_amount_ratio'] > 0.77]['merchant information'].tolist()\n",
    "\n",
    "        # Combine high risk mcc for each transaction type \n",
    "        high_risk_mccs_type = list(set(high_risk_mccs_amt + high_risk_mccs_fraud))\n",
    "        high_risk_mccs.extend(high_risk_mccs_type)\n",
    "    \n",
    "    # Combine mccs across all transaction types\n",
    "    high_risk_mccs = list(set(high_risk_mccs))\n",
    "    return high_risk_mccs\n",
    "\n",
    "#high_risk_mccs = high_risk_mcc_by_transact_type(df)\n",
    "\n",
    "#print(f'High risk merchants include: {high_risk_mccs}')\n",
    "\n",
    "# Create new column for high risk merchant\n",
    "#df['high_risk_merchant'] = df['merchant information'].isin(high_risk_mccs).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign risk score to each merchant based on fraud rate and transaction amount instead of a binary col that indicates if the merchant is high risk or not to capture more information about each merch\n",
    "def calculate_merchant_risk_score(df):\n",
    "    \"\"\"\n",
    "    Calculates a continuous risk score for each merchant based on fraud rate, high-value transaction frequency, and transaction amount statistics.\n",
    "    Returns: DataFrame with merchant information and risk scores.\n",
    "    \"\"\"\n",
    "    transaction_types = ['Chip Transaction', 'Online Transaction', 'Swipe Transaction']\n",
    "    merchant_risk_scores = []\n",
    "\n",
    "    for t in transaction_types:\n",
    "        # Split into respective transaction type\n",
    "        df_type = df[df['use_chip_' + t] == 1]\n",
    "\n",
    "        # Calculate fraud rate for each merchant\n",
    "        fraud_mcc = df_type.groupby('merchant information')['Target'].mean().reset_index()\n",
    "        fraud_mcc.rename(columns={'Target': 'fraud_rate'}, inplace=True)\n",
    "\n",
    "        # Calculate quartiles for amounts per merchant\n",
    "        mcc_quartiles = df_type.groupby('merchant information')['amount'].quantile([0.25, 0.75]).unstack()\n",
    "        mcc_quartiles.columns = ['Q1', 'Q3']\n",
    "        mcc_quartiles['IQR'] = mcc_quartiles['Q3'] - mcc_quartiles['Q1']\n",
    "        mcc_quartiles['amount_threshold'] = mcc_quartiles['Q3'] + 1.5 * mcc_quartiles['IQR']  # upper whisker value\n",
    "\n",
    "        # Count occurrence of high transaction amounts per merchant\n",
    "        df_merged = pd.merge(df_type, mcc_quartiles, on='merchant information', how='left')\n",
    "        df_merged['high_amount'] = (df_merged['amount'] >= df_merged['amount_threshold']).astype(int)\n",
    "        high_amount_counts = df_merged.groupby('merchant information')['high_amount'].sum().reset_index()\n",
    "        high_amount_counts.rename(columns={'high_amount': 'high_amount_count'}, inplace=True)\n",
    "\n",
    "        # Calculate the ratio of high transaction amount relative to total transactions\n",
    "        total_transaction_counts = df_type.groupby('merchant information').size().reset_index(name='total_transactions')\n",
    "        mcc_ratios = pd.merge(high_amount_counts, total_transaction_counts, on='merchant information')\n",
    "        mcc_ratios['high_amount_ratio'] = mcc_ratios['high_amount_count'] / mcc_ratios['total_transactions']\n",
    "\n",
    "        # Merge fraud rate and high-value transaction ratio\n",
    "        merchant_risk = pd.merge(fraud_mcc, mcc_ratios, on='merchant information', how='left')\n",
    "\n",
    "        # Normalize factors to a 0-1 scale\n",
    "        merchant_risk['fraud_rate_norm'] = (merchant_risk['fraud_rate'] - merchant_risk['fraud_rate'].min()) / (merchant_risk['fraud_rate'].max() - merchant_risk['fraud_rate'].min())\n",
    "        merchant_risk['high_amount_ratio_norm'] = (merchant_risk['high_amount_ratio'] - merchant_risk['high_amount_ratio'].min()) / (merchant_risk['high_amount_ratio'].max() - merchant_risk['high_amount_ratio'].min())\n",
    "\n",
    "        # Assign weights and calculate risk score for each merchant\n",
    "        weights = {'fraud_rate': 0.6, 'high_amount_ratio': 0.4}  # Adjust weights as needed\n",
    "        merchant_risk['risk_score'] = (\n",
    "            merchant_risk['fraud_rate_norm'] * weights['fraud_rate'] +\n",
    "            merchant_risk['high_amount_ratio_norm'] * weights['high_amount_ratio']\n",
    "        )\n",
    "\n",
    "        # Append to the list of merchant risk scores\n",
    "        merchant_risk_scores.append(merchant_risk[['merchant information', 'risk_score']])\n",
    "\n",
    "    # Combine risk scores across all transaction types\n",
    "    merchant_risk_scores = pd.concat(merchant_risk_scores).groupby('merchant information')['risk_score'].max().reset_index()\n",
    "\n",
    "    return merchant_risk_scores\n",
    "\n",
    "# Calculate merchant risk scores\n",
    "#merchant_risk_scores = calculate_merchant_risk_score(df)\n",
    "\n",
    "# Merge risk scores back into the main dataframe\n",
    "#df = pd.merge(df, merchant_risk_scores, on='merchant information', how='left')\n",
    "\n",
    "# Display the updated dataframe\n",
    "#print(df[['merchant information', 'risk_score']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved code chunk for merchant risk assessment\n",
    "def calculate_merchant_risk_score(df):\n",
    "    \"\"\"\n",
    "    Calculates a continuous risk score for each merchant based on fraud rate, high-value transaction frequency, and transaction amount statistics.\n",
    "    Returns: DataFrame with merchant information and risk scores.\n",
    "    \"\"\"\n",
    "    transaction_types = ['Chip Transaction', 'Online Transaction', 'Swipe Transaction']\n",
    "    merchant_risk_scores = []\n",
    "\n",
    "    for t in transaction_types:\n",
    "        # Split into respective transaction type\n",
    "        df_type = df[df['use_chip_' + t] == 1]\n",
    "\n",
    "        # Calculate fraud rate for each merchant\n",
    "        fraud_mcc = df_type.groupby('merchant information')['Target'].mean().reset_index()\n",
    "        fraud_mcc.rename(columns={'Target': 'fraud_rate'}, inplace=True)\n",
    "\n",
    "        # Calculate quartiles for amounts per merchant\n",
    "        mcc_quartiles = df_type.groupby('merchant information')['amount'].quantile([0.25, 0.75]).unstack()\n",
    "        mcc_quartiles.columns = ['Q1', 'Q3']\n",
    "        mcc_quartiles['IQR'] = mcc_quartiles['Q3'] - mcc_quartiles['Q1']\n",
    "        mcc_quartiles['amount_threshold'] = mcc_quartiles['Q3'] + 1.5 * mcc_quartiles['IQR']  # upper whisker value\n",
    "\n",
    "        # Count occurrence of high transaction amounts per merchant\n",
    "        df_merged = pd.merge(df_type, mcc_quartiles, on='merchant information', how='left')\n",
    "        df_merged['high_amount'] = (df_merged['amount'] >= df_merged['amount_threshold']).astype(int)\n",
    "        high_amount_counts = df_merged.groupby('merchant information')['high_amount'].sum().reset_index()\n",
    "        high_amount_counts.rename(columns={'high_amount': 'high_amount_count'}, inplace=True)\n",
    "\n",
    "        # Calculate the ratio of high transaction amount relative to total transactions\n",
    "        total_transaction_counts = df_type.groupby('merchant information').size().reset_index(name='total_transactions')\n",
    "        mcc_ratios = pd.merge(high_amount_counts, total_transaction_counts, on='merchant information')\n",
    "        mcc_ratios['high_amount_ratio'] = mcc_ratios['high_amount_count'] / mcc_ratios['total_transactions']\n",
    "\n",
    "        # Calculate average transaction amount per merchant\n",
    "        mcc_avg_amt = df_type.groupby('merchant information')['amount'].mean().reset_index()\n",
    "        mcc_avg_amt.rename(columns={'amount': 'avg_transaction_amt'}, inplace=True)\n",
    "\n",
    "        # Merge all merchant features\n",
    "        merchant_risk = pd.merge(fraud_mcc, mcc_ratios, on='merchant information', how='left')\n",
    "        merchant_risk = pd.merge(merchant_risk, mcc_avg_amt, on='merchant information', how='left')\n",
    "\n",
    "        # Normalize avg_transaction_amt\n",
    "        min_amt = merchant_risk['avg_transaction_amt'].min()\n",
    "        max_amt = merchant_risk['avg_transaction_amt'].max()\n",
    "        if max_amt > min_amt:\n",
    "            merchant_risk['avg_transaction_amt_norm'] = (merchant_risk['avg_transaction_amt'] - min_amt) / (max_amt - min_amt)\n",
    "        else:\n",
    "            merchant_risk['avg_transaction_amt_norm'] = 0  # Handle division by zero\n",
    "\n",
    "        # Assign weights and calculate risk score for each merchant\n",
    "        weights = {'fraud_rate': 0.5, 'high_amount_ratio': 0.3, 'avg_transaction_amt_norm': 0.2}  # Adjust weights as needed\n",
    "        merchant_risk['risk_score'] = (\n",
    "            merchant_risk['fraud_rate'] * weights['fraud_rate'] +\n",
    "            merchant_risk['high_amount_ratio'] * weights['high_amount_ratio'] +\n",
    "            merchant_risk['avg_transaction_amt_norm'] * weights['avg_transaction_amt_norm']\n",
    "        )\n",
    "\n",
    "        # Append to the list of merchant risk scores\n",
    "        merchant_risk_scores.append(merchant_risk[['merchant information', 'risk_score', 'total_transactions', 'avg_transaction_amt']])\n",
    "\n",
    "    # Combine risk scores across all transaction types\n",
    "    merchant_risk_scores = pd.concat(merchant_risk_scores).groupby('merchant information').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'risk_score': np.average(x['risk_score'], weights=x['total_transactions']),\n",
    "            'total_transactions': x['total_transactions'].sum(),\n",
    "            'avg_transaction_amt': np.average(x['avg_transaction_amt'], weights=x['total_transactions'])\n",
    "        })\n",
    "    ).reset_index()\n",
    "\n",
    "    # Fill missing values if any\n",
    "    merchant_risk_scores = merchant_risk_scores.fillna(0)\n",
    "\n",
    "    return merchant_risk_scores\n",
    "\n",
    "# Calculate merchant risk scores\n",
    "merchant_risk_scores = calculate_merchant_risk_score(df)\n",
    "\n",
    "# Merge risk scores back into the main dataframe\n",
    "df = pd.merge(df, merchant_risk_scores, on='merchant information', how='left')\n",
    "\n",
    "# Display the updated dataframe\n",
    "print(df[['merchant information', 'risk_score', 'total_transactions', 'avg_transaction_amt']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess from individual baseline purchase behaviours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess based on offline and in person transaction behaviours: in person to consider distance as well. To consider amount, time and day of purchase in each transaction type\n",
    "#!pip install geopy\n",
    "from geopy.geocoders import Nominatim # very slow, will change to photon through api\n",
    "from geopy.distance import geodesic\n",
    "import requests\n",
    "from sklearn.ensemble import IsolationForest\n",
    "#!pip install swifter\n",
    "import swifter\n",
    "\n",
    "# Precompute merchant coordinates\n",
    "#geolocator = Nominatim(user_agent=\"fraud_detection_app\")\n",
    "\n",
    "# Photon API endpoint\n",
    "PHOTON_API_URL = 'https://photon.komoot.io/api/'\n",
    "\n",
    "# Function to geocode a city using Photon API\n",
    "def geocode_city(city):\n",
    "    try:\n",
    "        response = requests.get(PHOTON_API_URL, params={\"q\": city, \"limit\": 1}) # return only 1 relevant result for each coordinate\n",
    "        if response.status_code == 200:\n",
    "            results = response.json()\n",
    "            if results['features']:\n",
    "                location = results['features'][0]['geometry']['coordinates']\n",
    "                return (location[1], location[0])  # Photon returns (lon, lat)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error geocoding {city}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get unique cities\n",
    "#unique_cities = df['merchant_city'].unique()\n",
    "#city_coords = {}\n",
    "\n",
    "#for city in unique_cities:\n",
    " #   coords = geocode_city(city)\n",
    "  #  if coords:\n",
    "   #     city_coords[city] = coords\n",
    "    #else:\n",
    "     #   print(f\"Could not geocode city: {city}\")\n",
    "\n",
    "# Save city_coords to a file\n",
    "#with open('city_coords.json', 'w') as f:\n",
    " #   json.dump(city_coords, f)\n",
    "\n",
    "# Load precomputed coordinates\n",
    "#with open('city_coords.json', 'r') as f:\n",
    " #   city_coords = json.load(f)\n",
    "\n",
    "# Define the distance calculation function\n",
    "def calculate_distance_merch(row, city_coords):\n",
    "    client_location = (row['latitude'], row['longitude'])\n",
    "    merchant_city = row['merchant_city']\n",
    "    \n",
    "    if merchant_city in city_coords:\n",
    "        merchant_coords = city_coords[merchant_city]\n",
    "        distance = geodesic(client_location, merchant_coords).km\n",
    "        return distance\n",
    "    else:\n",
    "        return None  # Handle missing coordinates\n",
    "\n",
    "'''\n",
    "COORDINATES OF MERCHANT CITY AND DISTANCE BETWEEN CLIENTS AND MERCHANT WILL NOT BE CALCULATED ANYMORE DUE TO THE NEED FOR OVERLY INTENSIVE COMPUTATIONAL RESOURCES \n",
    "'''\n",
    "\n",
    "def flag_potential_fraud_indiv(df):\n",
    "    '''Flags potential fraud transactions based on individual purchasing behaviour, considering online vs offline differences'''\n",
    "    \n",
    "    # --- Metrics regardless of transaction nature ---\n",
    "    \n",
    "    # Historical fraud rate per client\n",
    "    df['client_fraud_rate'] = df.groupby('client_id')['Target'].transform('mean')\n",
    "    \n",
    "    # Transaction frequency per client\n",
    "    df['client_transaction_freq'] = df.groupby('client_id')['amount'].transform('count')\n",
    "    \n",
    "    # Time since last transaction per client\n",
    "    df['time_since_last_txn'] = df.groupby('client_id')['date'].diff().dt.total_seconds() / 3600  # Convert to hours\n",
    "    df['time_since_last_txn'] = df['time_since_last_txn'].fillna(0)  # Fill 0 for first transaction\n",
    "\n",
    "    # Calculate financial ratios\n",
    "    df['amt_income_ratio'] = df['amount'] / df['yearly_income']\n",
    "    df['debt_income_ratio'] = df['total_debt'] / df['yearly_income']\n",
    "\n",
    "    # --- Split into Online and Offline Transactions ---\n",
    "    df_online = df[df['use_chip_Online Transaction'] == 1]\n",
    "    df_offline = df[df['use_chip_Online Transaction'] != 1]\n",
    "\n",
    "    # Function to map hours to categories\n",
    "    def hour_to_cat(hour):\n",
    "        if 6 <= hour < 12:\n",
    "            return 'Morning'\n",
    "        elif 12 <= hour < 17:\n",
    "            return 'Afternoon'\n",
    "        elif 17 <= hour < 22:\n",
    "            return 'Evening'\n",
    "        else:\n",
    "            return 'Night'\n",
    "\n",
    "    # --- Online Transactions: Additional Features ---\n",
    "    if not df_online.empty:\n",
    "        df_online['hour_category'] = df_online['transaction_hour'].apply(hour_to_cat)\n",
    "        # Calculate average transaction amount for online transactions\n",
    "        df_online['avg_transaction_amt'] = df_online.groupby('client_id')['amount'].transform('mean')\n",
    "        df_online['amt_avg_ratio'] = df_online['amount'] / df_online['avg_transaction_amt']\n",
    "\n",
    "        # Arrange transactions according to time and day for each user\n",
    "        df_online['weekend'] = df_online['transaction_day'].apply(lambda x: 1 if x >= 5 else 0)  # 5 for Sat, 6 for Sun\n",
    "        df_online['weekday_hour_category'] = df_online.apply(lambda row: f\"weekend_{row['hour_category']}\" if row['weekend'] == 1 else f\"weekday_{row['hour_category']}\", axis=1)\n",
    "\n",
    "        # Calculate % of transactions per weekday/weekend category combinations for each user\n",
    "        user_purchase_behaviour = df_online.groupby(['client_id', 'weekday_hour_category']).size().unstack(fill_value=0)\n",
    "        user_purchase_behaviour = user_purchase_behaviour.div(user_purchase_behaviour.sum(axis=1), axis=0)  # Normalize to %\n",
    "        df_online = df_online.merge(user_purchase_behaviour.stack().reset_index(name='weekday_hour_category_freq'), on=['client_id', 'weekday_hour_category'], how='left')\n",
    "\n",
    "        # Normalize features for typicality score\n",
    "        df_online['weekday_hour_category_freq_norm'] = (df_online['weekday_hour_category_freq'] - df_online['weekday_hour_category_freq'].min()) / (df_online['weekday_hour_category_freq'].max() - df_online['weekday_hour_category_freq'].min())\n",
    "        df_online['amt_avg_ratio_norm'] = (df_online['amt_avg_ratio'] - df_online['amt_avg_ratio'].min()) / (df_online['amt_avg_ratio'].max() - df_online['amt_avg_ratio'].min())\n",
    "        df_online['time_since_last_txn_norm'] = (df_online['time_since_last_txn'] - df_online['time_since_last_txn'].min()) / (df_online['time_since_last_txn'].max() - df_online['time_since_last_txn'].min())\n",
    "\n",
    "        # Define weights for typicality score\n",
    "        weights = {\n",
    "            'weekday_hour_category_freq': 0.4,  # Frequency-based feature\n",
    "            'amt_avg_ratio': 0.3,               # Transaction amount relative to average\n",
    "            'time_since_last_txn': 0.2,         # Time since last transaction\n",
    "            'client_fraud_rate': 0.1            # Historical fraud rate\n",
    "        }\n",
    "\n",
    "        # Calculate the weighted typicality score\n",
    "        df_online['typicality_score'] = (\n",
    "            (1 - df_online['weekday_hour_category_freq_norm']) * weights['weekday_hour_category_freq'] +\n",
    "            df_online['amt_avg_ratio_norm'] * weights['amt_avg_ratio'] +\n",
    "            df_online['time_since_last_txn_norm'] * weights['time_since_last_txn'] +\n",
    "            df_online['client_fraud_rate'] * weights['client_fraud_rate']\n",
    "        )\n",
    "\n",
    "        # Dummify hour category and weekday hour category\n",
    "        df_online = pd.get_dummies(df_online, columns=['hour_category'], prefix='hour', dtype=int)\n",
    "        df_online = pd.get_dummies(df_online, columns=['weekday_hour_category'], prefix='time_day', dtype=int)\n",
    "\n",
    "        # Flag anomalous transactions for online\n",
    "        df_online['suspicious_indiv_activity'] = 0\n",
    "        for client_id, client_data in df_online.groupby('client_id'):\n",
    "            features = [\n",
    "                'client_fraud_rate',\n",
    "                'client_transaction_freq',\n",
    "                'time_since_last_txn',\n",
    "                'amount',\n",
    "                'weekend',  # Include weekend feature\n",
    "                'typicality_score',\n",
    "                'amt_income_ratio',\n",
    "                'debt_income_ratio',\n",
    "                'amt_avg_ratio'\n",
    "            ] + [col for col in df_online.columns if col.startswith('hour_') or col.startswith('time_day_')]\n",
    "            \n",
    "            client_features = client_data[features]\n",
    "            isoforest = IsolationForest(contamination=0.05, random_state=42)\n",
    "            isoforest.fit(client_features)\n",
    "            anomaly_prediction = isoforest.predict(client_features)\n",
    "            df_online.loc[client_data.index, 'suspicious_indiv_activity'] = (anomaly_prediction == -1).astype(int)\n",
    "\n",
    "    # --- Offline Transactions: Additional Features ---\n",
    "    if not df_offline.empty:\n",
    "        df_offline['hour_category'] = df_offline['transaction_hour'].apply(hour_to_cat)\n",
    "        # Calculate average transaction amount for offline transactions\n",
    "        df_offline['avg_transaction_amt'] = df_offline.groupby('client_id')['amount'].transform('mean')\n",
    "        df_offline['amt_avg_ratio'] = df_offline['amount'] / df_offline['avg_transaction_amt']\n",
    "\n",
    "        # Arrange transactions according to time and day for each user\n",
    "        df_offline['weekend'] = df_offline['transaction_day'].apply(lambda x: 1 if x >= 5 else 0)  # 5 for Sat, 6 for Sun\n",
    "        df_offline['weekday_hour_category'] = df_offline.apply(lambda row: f\"weekend_{row['hour_category']}\" if row['weekend'] == 1 else f\"weekday_{row['hour_category']}\", axis=1)\n",
    "\n",
    "        # Calculate % of transactions per weekday/weekend category combinations for each user\n",
    "        user_purchase_behaviour = df_offline.groupby(['client_id', 'weekday_hour_category']).size().unstack(fill_value=0)\n",
    "        user_purchase_behaviour = user_purchase_behaviour.div(user_purchase_behaviour.sum(axis=1), axis=0)  # Normalize to %\n",
    "        df_offline = df_offline.merge(user_purchase_behaviour.stack().reset_index(name='weekday_hour_category_freq'), on=['client_id', 'weekday_hour_category'], how='left')\n",
    "\n",
    "        # Normalize features for typicality score\n",
    "        df_offline['weekday_hour_category_freq_norm'] = (df_offline['weekday_hour_category_freq'] - df_offline['weekday_hour_category_freq'].min()) / (df_offline['weekday_hour_category_freq'].max() - df_offline['weekday_hour_category_freq'].min())\n",
    "        df_offline['time_since_last_txn_norm'] = (df_offline['time_since_last_txn'] - df_offline['time_since_last_txn'].min()) / (df_offline['time_since_last_txn'].max() - df_offline['time_since_last_txn'].min())\n",
    "        df_offline['amt_avg_ratio_norm'] = (df_offline['amt_avg_ratio'] - df_offline['amt_avg_ratio'].min()) / (df_offline['amt_avg_ratio'].max() - df_offline['amt_avg_ratio'].min())\n",
    "\n",
    "        # Define weights for typicality score\n",
    "        weights = {\n",
    "            'weekday_hour_category_freq': 0.4,  # Frequency-based feature\n",
    "            'amt_avg_ratio': 0.3,               # Transaction amount relative to average\n",
    "            'time_since_last_txn': 0.2,         # Time since last transaction\n",
    "            'client_fraud_rate': 0.1            # Historical fraud rate\n",
    "        }\n",
    "\n",
    "        # Calculate the weighted typicality score\n",
    "        df_offline['typicality_score'] = (\n",
    "            (1 - df_offline['weekday_hour_category_freq_norm']) * weights['weekday_hour_category_freq'] +\n",
    "            df_offline['amt_avg_ratio_norm'] * weights['amt_avg_ratio'] +\n",
    "            df_offline['time_since_last_txn_norm'] * weights['time_since_last_txn'] +\n",
    "            df_offline['client_fraud_rate'] * weights['client_fraud_rate']\n",
    "        )\n",
    "\n",
    "        # Dummify hour category and weekday hour category\n",
    "        df_offline = pd.get_dummies(df_offline, columns=['hour_category'], prefix='hour', dtype=int)\n",
    "        df_offline = pd.get_dummies(df_offline, columns=['weekday_hour_category'], prefix='time_day', dtype=int)\n",
    "\n",
    "        # Flag anomalous transactions for offline\n",
    "        df_offline['suspicious_indiv_activity'] = 0\n",
    "        for client_id, client_data in df_offline.groupby('client_id'):\n",
    "            features = [\n",
    "                'client_fraud_rate',\n",
    "                'client_transaction_freq',\n",
    "                'time_since_last_txn',\n",
    "                'amount',\n",
    "                'weekend',  # Include weekend feature\n",
    "                'amt_income_ratio',\n",
    "                'debt_income_ratio',\n",
    "                'amt_avg_ratio',\n",
    "                'typicality_score'\n",
    "            ] + [col for col in df_offline.columns if col.startswith('hour_') or col.startswith('time_day_')]\n",
    "            \n",
    "            client_features = client_data[features]\n",
    "            isoforest = IsolationForest(contamination=0.05, random_state=42)\n",
    "            isoforest.fit(client_features)\n",
    "            anomaly_prediction = isoforest.predict(client_features)\n",
    "            df_offline.loc[client_data.index, 'suspicious_indiv_activity'] = (anomaly_prediction == -1).astype(int)\n",
    "\n",
    "    # --- Combine Results ---\n",
    "    df = pd.concat([df_online, df_offline], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "df = flag_potential_fraud_indiv(df)\n",
    "\n",
    "# Fill in missing values with 0\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Start of Model: XGBoost**\n",
    "\n",
    "Set up variables and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up variables\n",
    "df.rename(columns={'risk_score': 'merch_risk_score', 'total_transactions': 'merch_total_txn', 'avg_transaction_amt': 'avg_txn_merch'}, inplace=True)\n",
    "X = df.drop(df.columns[[0,1,3,4,5,8,9,10,16,33,34,44,46,47,48]], axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Stratified train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2, stratify=y, shuffle=True, random_state=42)\n",
    "\n",
    "# Verify fraud ratio in original data and in train and test\n",
    "print(\"Fraud ratio in original data: \", round(df['Target'].mean(), 3))\n",
    "print(\"Fraud ratio in train data: \", round(y_train.mean(), 3))\n",
    "print(\"Fraud ratio in test data: \", round(y_test.mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit base xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance in xgboost\n",
    "#scale_pos_wt = (y == 0).sum() / (y == 1).sum()\n",
    "\n",
    "# Initialise xgboost w scale pos wt\n",
    "#model = xgb.XGBClassifier(scale_pos_weight=scale_pos_wt, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "#y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "#print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "#print('\\nClassification Report:\\n', classification_report(y_test, y_pred))\n",
    "#print('\\nROC AUC Score:', roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define oversampling and undersampling\n",
    "over = SMOTE(sampling_strategy=0.2, random_state=42)  # Oversample fraud to 20%\n",
    "under = RandomUnderSampler(sampling_strategy=0.3, random_state=42)  # Balance to 1:3 ratio\n",
    "\n",
    "# Create pipeline\n",
    "imbalance_pipeline = Pipeline(steps=[(\"over\", over), (\"under\", under)])\n",
    "\n",
    "# Apply resampling\n",
    "X_resampled, y_resampled = imbalance_pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the shape of the split\n",
    "print(\"Train Set:\", X_resampled.shape, y_resampled.shape)\n",
    "print(\"Test Set:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Calculate scale_pos_weight directly\n",
    "#scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "scale_pos_weight = (y_resampled == 0).sum() / (y_resampled == 1).sum()\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "# Train XGBoost with scale_pos_weight\n",
    "model = XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,  # Handle class imbalance\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',  # Use logloss for binary classification\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for ROC-AUC\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot feature importance and SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model, importance_type='gain', max_num_features=15)\n",
    "plt.show()\n",
    "\n",
    "#explainer = shap.TreeExplainer(model)\n",
    "#shap_values = explainer.shap_values(X_train)\n",
    "#shap.summary_plot(shap_values, X_train, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'scale_pos_weight': [1, 2, 3, 5, 10, 20, 50, 100, 200],  # Adjust based on imbalance\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Define F2 score (beta=2 prioritizes recall)\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# RandomizedSearchCV for hyperparameter tuning with resampling\n",
    "def fit_model_with_resampling(X, y):\n",
    "    # Apply resampling\n",
    "    X_resampled, y_resampled = imbalance_pipeline.fit_resample(X, y)\n",
    "    \n",
    "    # Fit the model\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_clf,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=100,\n",
    "        scoring=f2_scorer,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit RandomizedSearchCV on resampled data\n",
    "    random_search.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    return random_search\n",
    "\n",
    "# Fit the model using the training data\n",
    "best_model = fit_model_with_resampling(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters found: \", best_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, y_pred))\n",
    "print('\\nROC AUC Score:', roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot feature importance and SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(best_model, importance_type='gain', max_num_features=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full code for xgboost tuning with ready prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "#!pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, f1_score, precision_score, recall_score\n",
    "\n",
    "prepared = pd.read_csv('/Users/jiajue/Documents/McGill/Winter Term/INSY695/Group project/prepared_data.csv')\n",
    "prepared.info()\n",
    "\n",
    "X = prepared.drop(['Target'], axis=1)\n",
    "y = prepared['Target']\n",
    "\n",
    "# Stratified train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2, stratify=y, shuffle=True, random_state=42)\n",
    "\n",
    "# Define oversampling and undersampling\n",
    "over = SMOTE(sampling_strategy=0.2, random_state=42)  # Oversample fraud to 20%\n",
    "under = RandomUnderSampler(sampling_strategy=0.3, random_state=42)  # Balance to 1:3 ratio\n",
    "\n",
    "# Create pipeline for resampling\n",
    "imbalance_pipeline = Pipeline(steps=[(\"over\", over), (\"under\", under)])\n",
    "\n",
    "# Apply resampling on the entire training set (just to check the shape of the dataset after resampling)\n",
    "X_resampled, y_resampled = imbalance_pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the shape of the split\n",
    "print(\"Train Set:\", X_resampled.shape, y_resampled.shape)\n",
    "print(\"Test Set:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Stratified train-test split for the initial training set (using y_train for stratification)\n",
    "X_train_tune, X_train_full, y_train_tune, y_train_full = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "# Calculate scale_pos_weight directly (for handling class imbalance)\n",
    "scale_pos_weight = (y_resampled == 0).sum() / (y_resampled == 1).sum()\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'scale_pos_weight': [1, 2, 3, 5, 10, 20, 50, 100, 200],  # Adjust based on imbalance\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Define F2 score (beta=2 prioritizes recall)\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# RandomizedSearchCV for hyperparameter tuning with resampling\n",
    "def fit_model_with_resampling(X, y):\n",
    "    # Apply resampling\n",
    "    X_resampled, y_resampled = imbalance_pipeline.fit_resample(X, y)\n",
    "    \n",
    "    # Fit the model\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_clf,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=100,\n",
    "        scoring=f2_scorer,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit RandomizedSearchCV on resampled data\n",
    "    random_search.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    return random_search\n",
    "\n",
    "# Perform hyperparameter tuning on a subset of the training data\n",
    "best_model = fit_model_with_resampling(X_train_tune, y_train_tune)\n",
    "\n",
    "# Print best parameters found\n",
    "print(\"Best parameters found: \", best_model.best_params_)\n",
    "\n",
    "# Now use the best model to train on the entire training data\n",
    "best_xgb_model = best_model.best_estimator_\n",
    "\n",
    "# Resample the full training data and fit the model\n",
    "X_train_full_resampled, y_train_full_resampled = imbalance_pipeline.fit_resample(X_train_full, y_train_full)\n",
    "\n",
    "# Train the final model on the full resampled training data\n",
    "best_xgb_model.fit(X_train_full_resampled, y_train_full_resampled)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_xgb_model.predict(X_test)\n",
    "y_pred_proba = best_xgb_model.predict_proba(X_test)[:, 1]  # Probabilities for ROC-AUC\n",
    "\n",
    "# Print classification report and AUC score\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install catboost\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Define oversampling and undersampling\n",
    "over = SMOTE(sampling_strategy=0.2, random_state=42)  # Oversample fraud to 20%\n",
    "under = RandomUnderSampler(sampling_strategy=0.3, random_state=42)  # Balance to 1:3 ratio\n",
    "\n",
    "# Create pipeline\n",
    "imbalance_pipeline = Pipeline(steps=[(\"over\", over), (\"under\", under)])\n",
    "\n",
    "# Apply resampling\n",
    "X_resampled, y_resampled = imbalance_pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the shape of the split\n",
    "print(\"Train Set:\", X_resampled.shape, y_resampled.shape)\n",
    "print(\"Test Set:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Calculate scale_pos_weight directly\n",
    "scale_pos_weight = (y_resampled == 0).sum() / (y_resampled == 1).sum()\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "# Train CatBoost with scale_pos_weight\n",
    "model = CatBoostClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,  # Handle class imbalance\n",
    "    random_state=42,\n",
    "    loss_function='Logloss',  # Binary classification with logloss\n",
    "    custom_metric=['AUC']  # For AUC scoring\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_resampled, y_resampled, verbose=0)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for ROC-AUC\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model to a file using joblib\n",
    "joblib.dump(model, 'catboost_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8914963 entries, 0 to 8914962\n",
      "Data columns (total 49 columns):\n",
      " #   Column                       Dtype  \n",
      "---  ------                       -----  \n",
      " 0   amount                       float64\n",
      " 1   Target                       int64  \n",
      " 2   current_age                  int64  \n",
      " 3   retirement_age               int64  \n",
      " 4   per_capita_income            float64\n",
      " 5   yearly_income                float64\n",
      " 6   total_debt                   float64\n",
      " 7   credit_score                 int64  \n",
      " 8   num_credit_cards             int64  \n",
      " 9   use_chip_Chip Transaction    int64  \n",
      " 10  use_chip_Online Transaction  int64  \n",
      " 11  use_chip_Swipe Transaction   int64  \n",
      " 12  merchant_state_CA            int64  \n",
      " 13  merchant_state_FL            int64  \n",
      " 14  merchant_state_IL            int64  \n",
      " 15  merchant_state_MI            int64  \n",
      " 16  merchant_state_NC            int64  \n",
      " 17  merchant_state_NY            int64  \n",
      " 18  merchant_state_OH            int64  \n",
      " 19  merchant_state_ONLINE        int64  \n",
      " 20  merchant_state_Other         int64  \n",
      " 21  merchant_state_PA            int64  \n",
      " 22  merchant_state_TX            int64  \n",
      " 23  gender_female                int64  \n",
      " 24  gender_male                  int64  \n",
      " 25  merch_risk_score             float64\n",
      " 26  merch_total_txn              float64\n",
      " 27  avg_txn_merch                float64\n",
      " 28  client_fraud_rate            float64\n",
      " 29  client_transaction_freq      int64  \n",
      " 30  time_since_last_txn          float64\n",
      " 31  amt_income_ratio             float64\n",
      " 32  debt_income_ratio            float64\n",
      " 33  amt_avg_ratio                float64\n",
      " 34  weekday_hour_category_freq   float64\n",
      " 35  typicality_score             float64\n",
      " 36  hour_Afternoon               int64  \n",
      " 37  hour_Evening                 int64  \n",
      " 38  hour_Morning                 int64  \n",
      " 39  hour_Night                   int64  \n",
      " 40  time_day_weekday_Afternoon   int64  \n",
      " 41  time_day_weekday_Evening     int64  \n",
      " 42  time_day_weekday_Morning     int64  \n",
      " 43  time_day_weekday_Night       int64  \n",
      " 44  time_day_weekend_Afternoon   int64  \n",
      " 45  time_day_weekend_Evening     int64  \n",
      " 46  time_day_weekend_Morning     int64  \n",
      " 47  time_day_weekend_Night       int64  \n",
      " 48  suspicious_indiv_activity    int64  \n",
      "dtypes: float64(14), int64(35)\n",
      "memory usage: 3.3 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "prepared = pd.read_csv('/Users/jiajue/Documents/McGill/Winter Term/INSY695/Group project/prepared_data.csv')\n",
    "prepared.info()\n",
    "\n",
    "X = prepared.drop(['Target'], axis=1)\n",
    "y = prepared['Target']\n",
    "\n",
    "# Stratified train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2, stratify=y, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: (6171793, 48) (6171793,)\n",
      "Test Set: (1782993, 48) (1782993,)\n",
      "scale_pos_weight: 3.3333330992936685\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "0:\tlearn: 0.5158076\ttotal: 3.29s\tremaining: 10m 55s\n",
      "0:\tlearn: 0.5168739\ttotal: 3.59s\tremaining: 11m 54s\n",
      "0:\tlearn: 0.5166381\ttotal: 3.69s\tremaining: 12m 13s\n",
      "0:\tlearn: 0.4893234\ttotal: 4.17s\tremaining: 17m 18s\n",
      "0:\tlearn: 0.4893206\ttotal: 4.1s\tremaining: 16m 59s\n",
      "0:\tlearn: 0.4892260\ttotal: 4.4s\tremaining: 18m 16s\n",
      "0:\tlearn: 0.4894937\ttotal: 4.44s\tremaining: 18m 26s\n",
      "0:\tlearn: 0.4882477\ttotal: 4.54s\tremaining: 18m 51s\n",
      "100:\tlearn: 0.0576329\ttotal: 4m 28s\tremaining: 4m 23s\n",
      "100:\tlearn: 0.0615989\ttotal: 4m 30s\tremaining: 4m 25s\n",
      "100:\tlearn: 0.0609439\ttotal: 4m 35s\tremaining: 4m 30s\n",
      "100:\tlearn: 0.0416718\ttotal: 5m 28s\tremaining: 8m 4s\n",
      "100:\tlearn: 0.0366333\ttotal: 5m 30s\tremaining: 8m 6s\n",
      "100:\tlearn: 0.0426028\ttotal: 5m 29s\tremaining: 8m 6s\n",
      "100:\tlearn: 0.0413751\ttotal: 5m 30s\tremaining: 8m 7s\n",
      "100:\tlearn: 0.0405981\ttotal: 5m 31s\tremaining: 8m 9s\n",
      "199:\tlearn: 0.0431979\ttotal: 8m 7s\tremaining: 0us\n",
      "199:\tlearn: 0.0476758\ttotal: 8m 10s\tremaining: 0us\n",
      "199:\tlearn: 0.0468551\ttotal: 8m 13s\tremaining: 0us\n",
      "0:\tlearn: 0.5171342\ttotal: 3.05s\tremaining: 10m 7s\n",
      "0:\tlearn: 0.5166189\ttotal: 2.87s\tremaining: 9m 30s\n",
      "0:\tlearn: 0.5129829\ttotal: 6.01s\tremaining: 9m 54s\n",
      "200:\tlearn: 0.0308884\ttotal: 10m 38s\tremaining: 2m 35s\n",
      "200:\tlearn: 0.0259165\ttotal: 10m 39s\tremaining: 2m 35s\n",
      "200:\tlearn: 0.0305561\ttotal: 10m 40s\tremaining: 2m 36s\n",
      "200:\tlearn: 0.0310412\ttotal: 10m 41s\tremaining: 2m 36s\n",
      "200:\tlearn: 0.0295819\ttotal: 10m 43s\tremaining: 2m 36s\n",
      "249:\tlearn: 0.0279203\ttotal: 12m 55s\tremaining: 0us\n",
      "249:\tlearn: 0.0274592\ttotal: 12m 56s\tremaining: 0us\n",
      "249:\tlearn: 0.0226271\ttotal: 12m 56s\tremaining: 0us\n",
      "100:\tlearn: 0.0619752\ttotal: 4m 24s\tremaining: 4m 19s\n",
      "249:\tlearn: 0.0270256\ttotal: 12m 58s\tremaining: 0us\n",
      "249:\tlearn: 0.0280108\ttotal: 12m 59s\tremaining: 0us\n",
      "100:\tlearn: 0.0619852\ttotal: 4m 22s\tremaining: 4m 16s\n",
      "0:\tlearn: 0.6788969\ttotal: 4.33s\tremaining: 7m 8s\n",
      "0:\tlearn: 0.4978201\ttotal: 7.78s\tremaining: 12m 50s\n",
      "0:\tlearn: 0.4977013\ttotal: 7.57s\tremaining: 12m 28s\n",
      "0:\tlearn: 0.4977587\ttotal: 7.96s\tremaining: 13m 8s\n",
      "0:\tlearn: 0.4979688\ttotal: 7.44s\tremaining: 12m 16s\n",
      "199:\tlearn: 0.0479724\ttotal: 8m 55s\tremaining: 0us\n",
      "199:\tlearn: 0.0471996\ttotal: 8m 54s\tremaining: 0us\n",
      "0:\tlearn: 0.6789989\ttotal: 3.62s\tremaining: 5m 58s\n",
      "0:\tlearn: 0.6789983\ttotal: 3.73s\tremaining: 6m 9s\n",
      "99:\tlearn: 0.0214718\ttotal: 10m 9s\tremaining: 0us\n",
      "0:\tlearn: 0.6790067\ttotal: 3.84s\tremaining: 6m 20s\n",
      "99:\tlearn: 0.2266343\ttotal: 6m 8s\tremaining: 0us\n",
      "0:\tlearn: 0.6790419\ttotal: 4.49s\tremaining: 7m 24s\n",
      "99:\tlearn: 0.0276281\ttotal: 10m 49s\tremaining: 0us\n",
      "99:\tlearn: 0.0276519\ttotal: 10m 51s\tremaining: 0us\n",
      "99:\tlearn: 0.2182726\ttotal: 6m 18s\tremaining: 0us\n",
      "99:\tlearn: 0.0277312\ttotal: 10m 50s\tremaining: 0us\n",
      "99:\tlearn: 0.0269366\ttotal: 10m 54s\tremaining: 0us\n",
      "99:\tlearn: 0.2242161\ttotal: 6m 17s\tremaining: 0us\n",
      "0:\tlearn: 0.4357110\ttotal: 3.52s\tremaining: 5m 48s\n",
      "0:\tlearn: 0.4375008\ttotal: 3.54s\tremaining: 5m 50s\n",
      "0:\tlearn: 0.4371919\ttotal: 3.96s\tremaining: 6m 32s\n",
      "0:\tlearn: 0.4377883\ttotal: 4.25s\tremaining: 7m\n",
      "0:\tlearn: 0.5107673\ttotal: 5.18s\tremaining: 21m 29s\n",
      "0:\tlearn: 0.4368381\ttotal: 4.58s\tremaining: 7m 33s\n",
      "99:\tlearn: 0.2208353\ttotal: 5m 54s\tremaining: 0us\n",
      "0:\tlearn: 0.5148937\ttotal: 4.63s\tremaining: 19m 12s\n",
      "99:\tlearn: 0.2271098\ttotal: 5m 48s\tremaining: 0us\n",
      "0:\tlearn: 0.5147531\ttotal: 4.46s\tremaining: 18m 29s\n",
      "99:\tlearn: 0.0261369\ttotal: 5m 39s\tremaining: 0us\n",
      "99:\tlearn: 0.0297249\ttotal: 5m 41s\tremaining: 0us\n",
      "99:\tlearn: 0.0307370\ttotal: 5m 42s\tremaining: 0us\n",
      "99:\tlearn: 0.0293270\ttotal: 5m 43s\tremaining: 0us\n",
      "99:\tlearn: 0.0312377\ttotal: 5m 44s\tremaining: 0us\n",
      "0:\tlearn: 0.5150939\ttotal: 5.95s\tremaining: 24m 42s\n",
      "0:\tlearn: 0.5150584\ttotal: 6.2s\tremaining: 25m 44s\n",
      "0:\tlearn: 0.5112110\ttotal: 6.13s\tremaining: 15m 13s\n",
      "0:\tlearn: 0.5054403\ttotal: 6.25s\tremaining: 15m 31s\n",
      "0:\tlearn: 0.5110655\ttotal: 6.3s\tremaining: 15m 38s\n",
      "100:\tlearn: 0.0232815\ttotal: 8m 1s\tremaining: 11m 50s\n",
      "100:\tlearn: 0.0281589\ttotal: 8m 5s\tremaining: 11m 55s\n",
      "100:\tlearn: 0.0289299\ttotal: 8m 15s\tremaining: 12m 10s\n",
      "100:\tlearn: 0.0275861\ttotal: 9m 56s\tremaining: 14m 40s\n",
      "100:\tlearn: 0.0278205\ttotal: 9m 59s\tremaining: 14m 44s\n",
      "100:\tlearn: 0.0210679\ttotal: 9m 58s\tremaining: 4m 50s\n",
      "100:\tlearn: 0.0273554\ttotal: 10m\tremaining: 4m 51s\n",
      "100:\tlearn: 0.0270968\ttotal: 9m 57s\tremaining: 4m 49s\n",
      "200:\tlearn: 0.0122758\ttotal: 17m 56s\tremaining: 4m 22s\n",
      "200:\tlearn: 0.0173271\ttotal: 18m 5s\tremaining: 4m 24s\n",
      "200:\tlearn: 0.0182568\ttotal: 18m 17s\tremaining: 4m 27s\n",
      "149:\tlearn: 0.0208787\ttotal: 14m 37s\tremaining: 0us\n",
      "149:\tlearn: 0.0150388\ttotal: 14m 36s\tremaining: 0us\n",
      "149:\tlearn: 0.0207968\ttotal: 14m 31s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5113630\ttotal: 4.85s\tremaining: 12m 3s\n",
      "0:\tlearn: 0.5113522\ttotal: 4.58s\tremaining: 11m 22s\n",
      "0:\tlearn: 0.6754248\ttotal: 3.94s\tremaining: 16m 21s\n",
      "249:\tlearn: 0.0096046\ttotal: 22m\tremaining: 0us\n",
      "0:\tlearn: 0.6755586\ttotal: 3.62s\tremaining: 15m\n",
      "249:\tlearn: 0.0146750\ttotal: 22m 2s\tremaining: 0us\n",
      "0:\tlearn: 0.6755756\ttotal: 3.78s\tremaining: 15m 41s\n",
      "249:\tlearn: 0.0153801\ttotal: 22m 7s\tremaining: 0us\n",
      "0:\tlearn: 0.6756094\ttotal: 3.65s\tremaining: 15m 9s\n",
      "200:\tlearn: 0.0171688\ttotal: 18m 36s\tremaining: 4m 32s\n",
      "200:\tlearn: 0.0171127\ttotal: 18m 36s\tremaining: 4m 32s\n",
      "100:\tlearn: 0.1487789\ttotal: 5m 51s\tremaining: 8m 37s\n",
      "100:\tlearn: 0.1609188\ttotal: 5m 58s\tremaining: 8m 48s\n",
      "249:\tlearn: 0.0147726\ttotal: 22m 24s\tremaining: 0us\n",
      "249:\tlearn: 0.0142924\ttotal: 22m 23s\tremaining: 0us\n",
      "0:\tlearn: 0.5309515\ttotal: 2.81s\tremaining: 6m 59s\n",
      "0:\tlearn: 0.6756075\ttotal: 3.8s\tremaining: 15m 46s\n",
      "100:\tlearn: 0.0266323\ttotal: 7m 55s\tremaining: 3m 50s\n",
      "100:\tlearn: 0.0260335\ttotal: 7m 56s\tremaining: 3m 51s\n",
      "100:\tlearn: 0.1617398\ttotal: 5m 52s\tremaining: 8m 39s\n",
      "100:\tlearn: 0.1540008\ttotal: 5m 48s\tremaining: 8m 34s\n",
      "149:\tlearn: 0.0207609\ttotal: 11m 40s\tremaining: 0us\n",
      "149:\tlearn: 0.0199102\ttotal: 11m 46s\tremaining: 0us\n",
      "100:\tlearn: 0.0588022\ttotal: 4m 25s\tremaining: 2m 8s\n",
      "200:\tlearn: 0.0880708\ttotal: 11m 43s\tremaining: 2m 51s\n",
      "0:\tlearn: 0.5319191\ttotal: 3.72s\tremaining: 9m 14s\n",
      "0:\tlearn: 0.5320385\ttotal: 3.31s\tremaining: 8m 12s\n",
      "200:\tlearn: 0.0930793\ttotal: 11m 46s\tremaining: 2m 52s\n",
      "100:\tlearn: 0.1614301\ttotal: 6m 10s\tremaining: 9m 6s\n",
      "200:\tlearn: 0.0949342\ttotal: 11m 54s\tremaining: 2m 54s\n"
     ]
    }
   ],
   "source": [
    "#!pip install catboost\n",
    "#!pip install imbalanced-learn\n",
    "\n",
    "# Define oversampling and undersampling\n",
    "over = SMOTE(sampling_strategy=0.2, random_state=42)  # Oversample fraud to 20%\n",
    "under = RandomUnderSampler(sampling_strategy=0.3, random_state=42)  # Balance to 1:3 ratio\n",
    "\n",
    "# Create pipeline for resampling\n",
    "imbalance_pipeline = Pipeline(steps=[(\"over\", over), (\"under\", under)])\n",
    "\n",
    "# Stratified train-test split for the initial training set (using y_train for stratification)\n",
    "X_train_tune, X_train_full, y_train_tune, y_train_full = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "# Apply resampling on the entire training set (just to check the shape of the dataset after resampling)\n",
    "X_resampled, y_resampled = imbalance_pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the shape of the split\n",
    "print(\"Train Set:\", X_resampled.shape, y_resampled.shape)\n",
    "print(\"Test Set:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Calculate scale_pos_weight directly (for handling class imbalance)\n",
    "scale_pos_weight = (y_resampled == 0).sum() / (y_resampled == 1).sum()\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning (adjusted for CatBoost)\n",
    "param_grid = {\n",
    "    'depth': [3, 5, 7, 10],  # Max depth for CatBoost\n",
    "    'min_data_in_leaf': [1, 3, 5],  # Minimum data in leaf\n",
    "    'subsample': [0.6, 0.8, 1.0],  # Subsample ratio\n",
    "    'colsample_bylevel': [0.6, 0.8, 1.0],  # Feature sampling ratio\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate\n",
    "    'iterations': [100, 150, 200, 250],  # Number of iterations\n",
    "    'l2_leaf_reg': [1, 3, 5, 10],  # L2 regularization term\n",
    "}\n",
    "\n",
    "# Define F2 score (beta=2 prioritizes recall)\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "# Initialize CatBoostClassifier without custom_metric\n",
    "catboost_clf = CatBoostClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,  # Initially set to 1.0 (you can adjust this later if needed)\n",
    "    random_state=42,\n",
    "    loss_function='Logloss',  # Binary classification with logloss\n",
    "    verbose=100  # Monitor training progress\n",
    ")\n",
    "\n",
    "# Define a function for hyperparameter tuning\n",
    "def fit_model_with_resampling(X, y):\n",
    "    # Apply resampling on the training subset\n",
    "    X_resampled, y_resampled = imbalance_pipeline.fit_resample(X, y)\n",
    "\n",
    "    # Fit the model with RandomizedSearchCV\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=catboost_clf,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=30,  # Number of iterations for RandomizedSearchCV\n",
    "        scoring=f2_scorer,\n",
    "        cv=5,  # Cross-validation\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "\n",
    "    # Fit RandomizedSearchCV on resampled data\n",
    "    random_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "    return random_search\n",
    "\n",
    "# Perform hyperparameter tuning on a subset of the training data\n",
    "best_model = fit_model_with_resampling(X_train_tune, y_train_tune)\n",
    "\n",
    "# Print best parameters found\n",
    "print(\"Best parameters found: \", best_model.best_params_)\n",
    "\n",
    "# Now use the best model to train on the entire training data\n",
    "best_catboost_model = best_model.best_estimator_\n",
    "\n",
    "# Resample the full training data and fit the model\n",
    "X_train_full_resampled, y_train_full_resampled = imbalance_pipeline.fit_resample(X_train_full, y_train_full)\n",
    "\n",
    "# Train the final model on the full resampled training data\n",
    "best_catboost_model.fit(X_train_full_resampled, y_train_full_resampled)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_catboost_model.predict(X_test)\n",
    "y_pred_proba = best_catboost_model.predict_proba(X_test)[:, 1]  # Probabilities for ROC-AUC\n",
    "\n",
    "# Print classification report and AUC score\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
